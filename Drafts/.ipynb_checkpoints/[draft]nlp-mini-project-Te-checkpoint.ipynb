{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b04cc596-0e06-4a9c-8be6-98a94ad68210",
   "metadata": {},
   "source": [
    "**Code pasted from NLP-Week2.2-Task Part4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "549a857f-fcc4-46b0-b0d1-25ddeb836ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.8/site-packages (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.8/site-packages (from beautifulsoup4) (2.2.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.8/site-packages (2.25.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: lxml in /opt/anaconda3/lib/python3.8/site-packages (4.6.3)\n"
     ]
    }
   ],
   "source": [
    "#Install neccessary packages\n",
    "\n",
    "!pip install beautifulsoup4\n",
    "!pip install requests\n",
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4471942c-0ccd-4dc5-a7ea-53fe56a5971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Get specific html and assign into an object\n",
    "url = \"https://www.azlyrics.com/lyrics/adele/easyonme.html\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "#Find the specific html tag\n",
    "song_lyrics = soup.find(class_=\"col-xs-12 col-lg-8 text-center\")\n",
    "song_lyrics_text = song_lyrics.find(\"div\",class_=\"\")\n",
    "\n",
    "#Remove all trivial text\n",
    "unwanted_brs = song_lyrics_text.find_all(\"br\")\n",
    "for i in range(len(unwanted_brs)):\n",
    "    unwanted_brs[i].decompose()\n",
    "\n",
    "#Show the final text we scraped\n",
    "song_lyrics_text.contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39953899-e6c5-41ab-866a-834372b5a194",
   "metadata": {},
   "source": [
    "**Code Pasted from NLP-Week4.1-Classifing Tweets-1/2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949ecf66-1d1c-4ade-9bf5-f658ba12cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install & import the twitter scraping library\n",
    "!pip install snscrape\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "\n",
    "#Initialise your data frame\n",
    "dataset = pd.DataFrame(columns = [\"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d8cfaa-756b-44e4-946c-6760d261cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "###YOU WILL HAVE TO RUN THIS CODE MULTIPLE TIMES WITH DIFFERENT QUERIES###\n",
    "###REMEMBER TO CHANGE THE LABEL WHEN YOU ARE ADDING TO A DIFFERENT CATEGORY###\n",
    "\n",
    "#How many to grab\n",
    "maxTweets = 300\n",
    "\n",
    "###Comment out methods below as appropriate \n",
    "\n",
    "# ---EITHER--- Keyword search\n",
    "# keyword = 'data'\n",
    "# data = sntwitter.TwitterSearchScraper(keyword + '-filter:links -filter:replies').get_items()\n",
    "\n",
    "# ---OR--- User search\n",
    "user = 'davidguetta'\n",
    "data = sntwitter.TwitterUserScraper(user).get_items()\n",
    "\n",
    "# ---OR--- Hastag search\n",
    "#hashtag = 'nlp'\n",
    "#data = sntwitter.TwitterHashtagScraper(hashtag + '-filter:links -filter:replies').get_items()\n",
    "\n",
    "#Set the label for this group\n",
    "label = 1\n",
    "new_examples = []\n",
    "#Collect in array and pair each tweet with a class label\n",
    "for i,tweet in enumerate(data):\n",
    "    if i >= maxTweets :\n",
    "        break    \n",
    "    new_examples = new_examples + [[tweet.content, label]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517eceeb-479d-4fd4-8ac6-f1ee146c95f0",
   "metadata": {},
   "source": [
    "**My sketch of code** first try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3068869f-05fb-4a56-9755-db81fc0a867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Api for douban\n",
    "http://api.douban.com/v2/user/1000001?apikey=XXX\n",
    "    \n",
    "#Api key\n",
    "0df993c66c0c636e29ecbb5344252a4a\n",
    "0b2bdeda43b5688921839c8ecb20399b\n",
    "\n",
    "#The fixed Api\n",
    "http://api.douban.com/v2/user/1000001?apikey=0df993c66c0c636e29ecbb5344252a4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a5777-877d-44ae-8826-b0eb9ec59f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A post on DOUBAN\n",
    "https://www.douban.com/group/topic/245062758/\n",
    "\n",
    "#Api for this post\n",
    "https://api.douban.com/v2/discussion/245062758?apikey=0df993c66c0c636e29ecbb5344252a4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9262d6e3-e775-4719-9a9e-72c928072dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd918a54-92d6-4974-8d1b-cfb1de78f322",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get specific html and assign into an object\n",
    "url = \"https://www.douban.com/group/topic/245062758/\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text,\"html.parser\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4cca81f-19e2-4583-bb7b-031251fa557a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [418]>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6cd4ea-9d06-40bb-8601-3d2bdbbc3df9",
   "metadata": {},
   "source": [
    "**Code from GitHub**\n",
    "https://github.com/FelixMundial/SimpleDoubanScraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da907e44-f54b-4a24-908e-93ec6a0e89f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    ":keyword web scraping on douban books\n",
    "\"\"\"\n",
    "import importlib\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import ssl\n",
    "import sys\n",
    "import time\n",
    "from urllib.error import HTTPError, URLError\n",
    "from urllib.parse import quote\n",
    "from urllib.request import Request, urlopen, ProxyHandler, build_opener, install_opener, HTTPHandler\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from numpy import random\n",
    "\n",
    "JSON_FILE_PATH = 'docs'\n",
    "RETRYING_TIMES = 20\n",
    "\n",
    "PLACEHOLDER_VALUE_NOT_SET = '暂无'\n",
    "SUFFIX_JSON = '.json'\n",
    "\n",
    "importlib.reload(sys)\n",
    "\n",
    "# Some Predefined User Agents\n",
    "USER_AGENTS = [\n",
    "    {'User-Agent': 'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (Windows NT 6.2) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.12 Safari/535.11'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; Trident/6.0)'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:73.0) Gecko/20100101 Firefox/73.0'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1 Safari/605.1.15'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:23.0) Gecko/20100101 Firefox/23.0'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:30.0) Gecko/20100101 Firefox/30.0'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; Win64; x64; Trident/6.0)'},\n",
    "    {'User-Agent': 'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (compatible; Konqueror/3.5; Linux) KHTML/3.5.5 (like Gecko) (Kubuntu)'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.0.12) Gecko/20070731 Ubuntu/dapper-security Firefox/1.5.0.12'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.7 (KHTML, like Gecko) Ubuntu/11.04 Chromium/16.0.912.77 Chrome/16.0.912.77 Safari/535.7'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:10.0) Gecko/20100101 Firefox/10.0'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36'}]\n",
    "\n",
    "PROXY_IPS = []\n",
    "\n",
    "\n",
    "def do_crawling_by_tag(book_tag, page_size, is_basic_tag):\n",
    "    \"\"\"\n",
    "    根据豆瓣标签页类型及具体标签进行实际的数据爬取\n",
    "    :param book_tag: 图书标签\n",
    "    :param page_size: 需要爬取的页数\n",
    "    :param is_basic_tag: 是否为豆瓣原生标签\n",
    "    :return: 爬取的数据集（含有数据信息字典的一维列表）\n",
    "    \"\"\"\n",
    "    page_count = 0\n",
    "    book_info_map_list = []\n",
    "    retrying_times = 0\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "    while 1:\n",
    "        if page_count == page_size:\n",
    "            break\n",
    "        print('Retrieving Information From Page %d' % (page_count + 1))\n",
    "        if is_basic_tag:\n",
    "            # 豆瓣读书预设标签列表\n",
    "            # book_url='http://www.douban.com/tag/%E5%B0%8F%E8%AF%B4/book?start=0'\n",
    "            # 单页15本书，start值为书的行数偏移量\n",
    "            url = 'http://www.douban.com/tag/' + quote(book_tag) \\\n",
    "                  + '/book?start=' + str(page_count * 15)\n",
    "        else:\n",
    "            # 豆瓣读书用户自定义标签列表（更丰富）\n",
    "            # book_url=''http://book.douban.com/tag/%E5%8F%A4%E5%B7%B4%E6%AF%94%E4%BC%A6?start=0&type=T''\n",
    "            # 单页15本书，start值为书的行数偏移量\n",
    "            url = 'http://book.douban.com/tag/' + quote(book_tag) \\\n",
    "                  + '?start=' + str(page_count * 15) + '&type=T'\n",
    "        time.sleep(random.random() * 8)\n",
    "\n",
    "        try:\n",
    "            # proxy = ProxyHandler({'http': random.choice(PROXY_IPS)})\n",
    "            # opener = build_opener(proxy, HTTPHandler)\n",
    "            # opener.addheaders = [('User-Agent', random.choice(USER_AGENTS).get('User-Agent'))]\n",
    "            # install_opener(opener)\n",
    "            headers = random.choice(USER_AGENTS)\n",
    "\n",
    "            # req = get(url, proxies={'http': random.choice(PROXY_IPS)}, headers=USER_AGENTS[np.random.randint(0, len(USER_AGENTS))])\n",
    "            req = Request(url, headers=headers)\n",
    "            source_code = urlopen(req).read()\n",
    "            plain_text = str(source_code, 'utf-8')\n",
    "        except (HTTPError, URLError) as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(plain_text, features=\"lxml\")\n",
    "\n",
    "        if is_basic_tag:\n",
    "            div_book_list = soup.find('div', {'class': 'mod book-list'})\n",
    "        else:\n",
    "            div_book_list = soup.find('ul', {'class': 'subject-list'})\n",
    "\n",
    "        retrying_times += 1\n",
    "        # 无有效标签数据\n",
    "        if div_book_list is None and page_count == 0:\n",
    "            # 1. 标签错误，该标签下无有效内容\n",
    "            if plain_text is not None:\n",
    "                print(\"找不到任何图书...请确认所指定的标签是否正确或存在\")\n",
    "                break\n",
    "            # 2. 连接错误，尝试重连\n",
    "            if retrying_times < RETRYING_TIMES:\n",
    "                print(\"尝试重新从豆瓣服务器获取响应...\")\n",
    "                continue\n",
    "        if div_book_list is None or len(div_book_list) <= 1:\n",
    "            break\n",
    "\n",
    "        if is_basic_tag:\n",
    "            for book_info in div_book_list.findAll('dd'):\n",
    "                title = book_info.find('a', {'class': 'title'}).string.strip()\n",
    "                print(title)\n",
    "                desc = book_info.find('div', {'class': 'desc'}).string.strip()\n",
    "                desc_list = desc.split('/')\n",
    "                book_url = book_info.find('a', {'class': 'title'}).get('href')\n",
    "\n",
    "                book_info = get_book_info(book_url)\n",
    "                try:\n",
    "                    author_info = '/'.join(desc_list[0:-3])\n",
    "                except:\n",
    "                    author_info = PLACEHOLDER_VALUE_NOT_SET\n",
    "                try:\n",
    "                    pub_info = '/'.join(desc_list[-3:])\n",
    "                except:\n",
    "                    pub_info = PLACEHOLDER_VALUE_NOT_SET\n",
    "                # try:\n",
    "                #     rating = book_info.find('span', {'class': 'rating_nums'}).string.strip()\n",
    "                # except:\n",
    "                #     rating = '0.0'\n",
    "                # try:\n",
    "                #     # raters_count = book_info.findAll('span')[2].string.strip()\n",
    "                #     raters_count = get_book_info(book_url).get('ratersCount').strip('人评价')\n",
    "                # except:\n",
    "                #     raters_count = '0'\n",
    "                book_comments = get_book_comments(book_url, False, 10)\n",
    "\n",
    "                print(\"authorInfo:\" + author_info + \"; rating:\" + str(book_info.get('averageRating')) + \"(\" + str(\n",
    "                    book_info.get('ratersCount')) + \")\" + \"; pubInfo:\" + pub_info + \"; img:\" + book_info.get('img')[0])\n",
    "                book_info_map = {'title': title, 'bookUrl': book_url, 'authorInfo': author_info, 'pubInfo': pub_info,\n",
    "                                 'bookInfo': book_info, 'comments': book_comments}\n",
    "                book_info_map_list.append(book_info_map)\n",
    "        else:\n",
    "            for book_info in div_book_list.find_all('div', {'class': 'info'}):\n",
    "                # 去掉标题区域的制表符\n",
    "                title = re.sub(r'\\s*', \"\", book_info.find('a').text.strip())\n",
    "                print(title)\n",
    "                desc = book_info.find('div', {'class': 'pub'}).string.strip()\n",
    "                desc_list = desc.split('/')\n",
    "                book_url = book_info.find('a').get('href')\n",
    "\n",
    "                book_info = get_book_info(book_url)\n",
    "                try:\n",
    "                    author_info = '/'.join(desc_list[0:-3])\n",
    "                except:\n",
    "                    author_info = PLACEHOLDER_VALUE_NOT_SET\n",
    "                try:\n",
    "                    pub_info = '/'.join(desc_list[-3:])\n",
    "                except:\n",
    "                    pub_info = PLACEHOLDER_VALUE_NOT_SET\n",
    "                # try:\n",
    "                #     rating = book_info.find('span', {'class': 'rating_nums'}).string.strip()\n",
    "                # except:\n",
    "                #     rating = '0.0'\n",
    "                # try:\n",
    "                #     # raters_count = book_info.find('span', {'class': 'pl'}).string.strip('人评价')\n",
    "                #     raters_count = get_book_info(book_url).get('ratersCount').strip('人评价')\n",
    "                # except:\n",
    "                #     raters_count = '0'\n",
    "                book_comments = get_book_comments(book_url, False, 10)\n",
    "\n",
    "                print(\"authorInfo:\" + author_info + \"; rating:\" + str(book_info.get('averageRating')) + \"(\" + str(\n",
    "                    book_info.get('ratersCount')) + \")\" + \"; pubInfo:\" + pub_info + \"; img:\" + book_info.get('img')[0])\n",
    "                book_info_map = {'title': title, 'bookUrl': book_url, 'authorInfo': author_info, 'pubInfo': pub_info,\n",
    "                                 'bookInfo': book_info, 'comments': book_comments}\n",
    "                book_info_map_list.append(book_info_map)\n",
    "\n",
    "            retrying_times = 0  # Reset when valid information is received\n",
    "\n",
    "        page_count += 1\n",
    "\n",
    "    return book_info_map_list\n",
    "\n",
    "\n",
    "def crawl_by_basic_tag(tag, page_size):\n",
    "    \"\"\"\n",
    "    爬取预设标签列表数据\n",
    "    :param tag: 图书标签\n",
    "    \"\"\"\n",
    "    return do_crawling_by_tag(tag, page_size, True)\n",
    "\n",
    "\n",
    "def crawl_by_user_tag(tag, page_size):\n",
    "    \"\"\"\n",
    "    爬取用户自定义标签列表数据\n",
    "    :param tag: 图书标签\n",
    "    \"\"\"\n",
    "    return do_crawling_by_tag(tag, page_size, False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7fb041-05c4-43be-b544-c8cfc54ad8b0",
   "metadata": {},
   "source": [
    "**Simplized Version of Github Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6040c380-9f65-43f9-b8ec-7534e658bb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First make sure we got pkgs we need\n",
    "#!pip install bs4\n",
    "#!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea8e9d0-5d9a-4028-9e7b-3eba7b5a0a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install all pkg we are going to use\n",
    "import ssl\n",
    "import os\n",
    "import re\n",
    "from urllib.error import HTTPError, URLError\n",
    "from urllib.parse import quote\n",
    "from urllib.request import Request, urlopen, ProxyHandler, build_opener, install_opener, HTTPHandler\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from numpy import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d5febb-72a6-44f1-8ed8-4439a733bac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating User Agents as Header when request a webpage\n",
    "USER_AGENTS = [\n",
    "    {'User-Agent': 'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (Windows NT 6.2) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.12 Safari/535.11'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; Trident/6.0)'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:73.0) Gecko/20100101 Firefox/73.0'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1 Safari/605.1.15'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:23.0) Gecko/20100101 Firefox/23.0'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:30.0) Gecko/20100101 Firefox/30.0'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; Win64; x64; Trident/6.0)'},\n",
    "    {'User-Agent': 'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (compatible; Konqueror/3.5; Linux) KHTML/3.5.5 (like Gecko) (Kubuntu)'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.0.12) Gecko/20070731 Ubuntu/dapper-security Firefox/1.5.0.12'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.7 (KHTML, like Gecko) Ubuntu/11.04 Chromium/16.0.912.77 Chrome/16.0.912.77 Safari/535.7'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:10.0) Gecko/20100101 Firefox/10.0'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36'}]\n",
    "\n",
    "#Using proxy in case there's limitation for single computer\n",
    "PROXY_IPS = []\n",
    "\n",
    "#Setting the url format\n",
    "# 豆瓣读书预设标签列表\n",
    "# book_url='http://www.douban.com/tag/%E5%B0%8F%E8%AF%B4/book?start=0'\n",
    "# 单页15本书，start值为书的行数偏移量\n",
    "url = 'http://www.douban.com/tag/%E5%B0%8F%E8%AF%B4/book?start=0'\n",
    "\n",
    "#Make the header randomly\n",
    "headers = random.choice(USER_AGENTS)\n",
    "\n",
    "#request and transfer into plain text for bs4 process\n",
    "req = Request(url, headers=headers)\n",
    "source_code = urlopen(req).read()\n",
    "plain_text = str(source_code, 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d774055-db80-4645-9d91-c5228e9964df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
