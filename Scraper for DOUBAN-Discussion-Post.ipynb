{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39d3dbbf-ca10-48cb-836e-15006554ea2e",
   "metadata": {},
   "source": [
    "### Package import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640f9612-6354-4fc4-95e3-9aa14b8dcc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caculation\n",
    "import numpy as np\n",
    "import re\n",
    "from numpy import random\n",
    "\n",
    "# Scraping\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import urllib\n",
    "from urllib.request import Request, urlopen, ProxyHandler, build_opener, install_opener, HTTPHandler\n",
    "from urllib.error import HTTPError, URLError\n",
    "from urllib.parse import quote\n",
    "\n",
    "# Show Results\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f916d73-1139-4add-9b58-59b7367ad7a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a04577-dcd4-40c2-9c45-e8bceb9d0a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Referenced & Edited from\n",
    "# https://github.com/FelixMundial/SimpleDoubanScraper/tree/7e779d717a50a8fff68752ac90d00c26109a3468\n",
    "\n",
    "#Setting User Agents\n",
    "\n",
    "USER_AGENTS = [\n",
    "    {'User-Agent': 'Mozilla/5.0 (Linux; Android 7.1.1; SM-A530F Build/NMF26X; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/64.0.3282.137 Mobile Safari/537.36 YandexSearch/7.21'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (Windows NT 6.2) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.12 Safari/535.11'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; Trident/6.0)'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:73.0) Gecko/20100101 Firefox/73.0'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (Linux; Android 10; P40) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/15.0 Chrome/90.0.4430.210 Mobile Safari/537.36'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:23.0) Gecko/20100101 Firefox/23.0'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:30.0) Gecko/20100101 Firefox/30.0'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8) AppleWebKit/5331 (KHTML, like Gecko) Chrome/37.0.840.0 Mobile Safari/5331'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (Linux; Android 9; Redmi Note 7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4603.0 Mobile Safari/537.36'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (compatible; Konqueror/3.5; Linux) KHTML/3.5.5 (like Gecko) (Kubuntu)'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.0.12) Gecko/20070731 Ubuntu/dapper-security Firefox/1.5.0.12'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.7 (KHTML, like Gecko) Ubuntu/11.04 Chromium/16.0.912.77 Chrome/16.0.912.77 Safari/535.7'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:10.0) Gecko/20100101 Firefox/10.0'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63a06ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape Free Proxy Site\n",
    "\n",
    "url = \"https://free-proxy-list.net/anonymous-proxy.html\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text,\"html.parser\")\n",
    "proxy_df = soup.find(\"tbody\")\n",
    "proxy_single = proxy_df.find_all(\"tr\")\n",
    "\n",
    "#Set Proxies\n",
    "\n",
    "proxy_list = []\n",
    "for index in range(len(proxy_single)):\n",
    "    if proxy_single[index].find_all(\"td\")[6].text == \"yes\":\n",
    "        address = str(proxy_single[index].find_all(\"td\")[0].text)\n",
    "        port = str(proxy_single[index].find_all(\"td\")[1].text)\n",
    "        #proxy_list.append(str(\"https://\" + address + \":\" + port))\n",
    "        proxy_list.append(str(address + \":\" + port))\n",
    "PROXY_IPS = []\n",
    "for index in range(len(proxy_list)):\n",
    "    PROXY_IPS.append({\"http\": proxy_list[index]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c863ee-b8cd-4bed-8e8a-5e79543c2826",
   "metadata": {},
   "source": [
    "### Scraping Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72c020d-e966-4182-8b0d-477fea0d98df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Request certain group discussion page\n",
    "\n",
    "#First make the url for every page\n",
    "\n",
    "group_discussion_url = 'http://www.douban.com/group/nopua/discussion?start='\n",
    "group_disscusion_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcafdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Referenced & Edited from\n",
    "# https://www.scrapehero.com/how-to-rotate-proxies-and-ip-addresses-using-python-3/\n",
    "\n",
    "#Make Header & Proxie\n",
    "headers = random.choice(USER_AGENTS)\n",
    "proxies = random.choice(PROXY_IPS)\n",
    "print(\"Connect to: \" + str(proxies))\n",
    "print(\"Header:\" + str(headers))\n",
    "print(\"------\")\n",
    "\n",
    "#Request & transfer the page (referenced from Github)\n",
    "group_discussion_url = group_discussion_url + str(group_disscusion_number)\n",
    "group_dis_req = requests.get(group_discussion_url, proxies=proxies, headers=headers)\n",
    "group_dis_page_text = BeautifulSoup(group_dis_req.text,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb05b37-26f5-4b30-bdc7-fdbc11880e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get certain group discussion post link into a dictionary\n",
    "\n",
    "grp_dis_list = {}\n",
    "list = group_dis_page_text.find_all(\"tr\")\n",
    "for i in list:\n",
    "    if i[\"class\"] == []:\n",
    "        title = str(i.a[\"title\"])\n",
    "        link = i.a[\"href\"]\n",
    "        grp_dis_list[title]=link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a96c8d-590f-491b-8928-aedf9e221dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape Every Post\n",
    "\n",
    "post_df = pd.DataFrame(columns = [\"title\",\"link\",\"main text\",\"reply text\"])\n",
    "for i in grp_dis_list:\n",
    "    \n",
    "    print(\"Scraping post: \", i )\n",
    "    \n",
    "    #Request & Get the html\n",
    "    grp_dis_list[i] #get the url\n",
    "    headers = random.choice(USER_AGENTS) #make a fake user\n",
    "    proxies = random.choice(PROXY_IPS)\n",
    "    req = requests.get(grp_dis_list[i], proxies=proxies, headers=headers)\n",
    "    soup = BeautifulSoup(req.text,\"html.parser\")\n",
    "    print(\"Connect to: \" + str(proxies))\n",
    "    print(\"Header:\" + str(headers))\n",
    "    print(\"------\")\n",
    "    \n",
    "    # Post Title / Link\n",
    "    post = [[]]\n",
    "    post[0].append(i)\n",
    "    post[0].append(grp_dis_list[i])\n",
    "    \n",
    "    # Post Main Text\n",
    "    # get main post\n",
    "    post_main = soup.find(\"div\", class_=\"rich-content topic-richtext\").find_all(\"p\")\n",
    "    # transfer main post object into a list of strings\n",
    "    post_main_text = []\n",
    "    for main in range(len(post_main)):\n",
    "        post_main_text.append(post_main[main].text)\n",
    "    \n",
    "    # Post Reply Text\n",
    "    #transfer reply post object into a list of strings\n",
    "    post_reply = soup.find_all(class_=\"reply-content\")\n",
    "    post_reply_text = []\n",
    "    for reply in range(len(post_reply)):\n",
    "        post_reply_text.append(post_reply[reply].text)\n",
    "    \n",
    "    # Make the Dataset\n",
    "    # add single post to data frame\n",
    "    post[0].append(post_main_text)\n",
    "    post[0].append(post_reply_text)\n",
    "    post_df = post_df.append(pd.DataFrame(post, columns = [\"title\",\"link\",\"main text\",\"reply text\"]))\n",
    "    \n",
    "    sleep_time = random.randint(10,20)\n",
    "    print(\"Now sleeping for \" + str(sleep_time) + \"s...\")\n",
    "    time.sleep(sleep_time) # modified from Github Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62453353-5090-4b07-a84f-1d4c66b491a9",
   "metadata": {},
   "source": [
    "### Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1dc8fc-6305-4e7b-bc6e-d8bd9baef1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write to file\n",
    "file_name = \"DOUBAN_dataset_group03\"\n",
    "post_df.to_csv(file_name + \".tsv\", sep = \"\\t\", columns = [\"title\",\"link\",\"main text\",\"reply text\"], index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
